{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**easy linear regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|explaining|label|\n",
      "+----------+-----+\n",
      "|       1.0|  5.0|\n",
      "|       2.0| 10.0|\n",
      "|       3.0| 15.0|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"Regression\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# load csv\n",
    "reg_data = spark.read.csv(\"../data/regression.csv\")\n",
    "new_name = [\"explaining\", \"label\"]\n",
    "reg_data = reg_data.toDF(*new_name)\n",
    "reg_data = reg_data.select([reg_data[col].cast(\"float\").alias(col) for col in reg_data.columns])\n",
    "reg_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. To the reg_data, make linear regression model with condition that the column 'explaining' is explaining variable and the column 'label' is explained variable.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"explaining\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "transformed_data = assembler.transform(reg_data)\n",
    "\n",
    "lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(transformed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. By the model you made above, do prediction to the data you used for the training.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------+------------------+\n",
      "|explaining|label|features|        prediction|\n",
      "+----------+-----+--------+------------------+\n",
      "|       1.0|  5.0|   [1.0]| 5.362101675379325|\n",
      "|       2.0| 10.0|   [2.0]|              10.0|\n",
      "|       3.0| 15.0|   [3.0]|14.637898324620675|\n",
      "+----------+-----+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lrModel.transform(transformed_data).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**logistic regression with iris dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Make Logistic regression model to iris data with standardization and proper pre-processing without Pipeline. The iris data must be splitted into train and test ones. Do prediction to the test data. As a first trial, it is okay to split the data into train and test ones just before the model training.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|species|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "|         5.1|        3.5|         1.4|        0.2| setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2| setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2| setosa|\n",
      "|         4.6|        3.1|         1.5|        0.2| setosa|\n",
      "|         5.0|        3.6|         1.4|        0.2| setosa|\n",
      "|         5.4|        3.9|         1.7|        0.4| setosa|\n",
      "|         4.6|        3.4|         1.4|        0.3| setosa|\n",
      "|         5.0|        3.4|         1.5|        0.2| setosa|\n",
      "|         4.4|        2.9|         1.4|        0.2| setosa|\n",
      "|         4.9|        3.1|         1.5|        0.1| setosa|\n",
      "|         5.4|        3.7|         1.5|        0.2| setosa|\n",
      "|         4.8|        3.4|         1.6|        0.2| setosa|\n",
      "|         4.8|        3.0|         1.4|        0.1| setosa|\n",
      "|         4.3|        3.0|         1.1|        0.1| setosa|\n",
      "|         5.8|        4.0|         1.2|        0.2| setosa|\n",
      "|         5.7|        4.4|         1.5|        0.4| setosa|\n",
      "|         5.4|        3.9|         1.3|        0.4| setosa|\n",
      "|         5.1|        3.5|         1.4|        0.3| setosa|\n",
      "|         5.7|        3.8|         1.7|        0.3| setosa|\n",
      "|         5.1|        3.8|         1.5|        0.3| setosa|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load iris\n",
    "iris_data = spark.read.csv(\"../data/iris.csv\")\n",
    "new_name = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"species\"]\n",
    "iris_data = iris_data.toDF(*new_name)\n",
    "iris_data = iris_data.select([iris_data[col].cast(\"float\").alias(col) for col in iris_data.columns[:-1]] + [iris_data['species']])\n",
    "iris_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler_lr = VectorAssembler(inputCols=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"], \n",
    "                               outputCol=\"raw_features\"\n",
    "                              )\n",
    "\n",
    "assembled_iris = assembler_lr.transform(iris_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"raw_features\",outputCol=\"features\", withStd=True, withMean=True)\n",
    "scaled_iris = scaler.fit(assembled_iris).transform(assembled_iris)\n",
    "\n",
    "string_indexer = StringIndexer(inputCol=\"species\", outputCol=\"label\")\n",
    "string_species_iris = string_indexer.fit(scaled_iris).transform(scaled_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+----------+--------------------+--------------------+-----+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|   species|        raw_features|            features|label|\n",
      "+------------+-----------+------------+-----------+----------+--------------------+--------------------+-----+\n",
      "|         4.4|        3.2|         1.3|        0.2|    setosa|[4.40000009536743...|[-1.7430169031234...|  2.0|\n",
      "|         4.5|        2.3|         1.3|        0.3|    setosa|[4.5,2.2999999523...|[-1.6222537139525...|  2.0|\n",
      "|         4.6|        3.1|         1.5|        0.2|    setosa|[4.59999990463256...|[-1.5014905247816...|  2.0|\n",
      "|         4.6|        3.2|         1.4|        0.2|    setosa|[4.59999990463256...|[-1.5014905247816...|  2.0|\n",
      "|         4.6|        3.4|         1.4|        0.3|    setosa|[4.59999990463256...|[-1.5014905247816...|  2.0|\n",
      "|         4.6|        3.6|         1.0|        0.2|    setosa|[4.59999990463256...|[-1.5014905247816...|  2.0|\n",
      "|         4.7|        3.2|         1.3|        0.2|    setosa|[4.69999980926513...|[-1.3807273356108...|  2.0|\n",
      "|         4.7|        3.2|         1.6|        0.2|    setosa|[4.69999980926513...|[-1.3807273356108...|  2.0|\n",
      "|         4.8|        3.0|         1.4|        0.1|    setosa|[4.80000019073486...|[-1.2599635705956...|  2.0|\n",
      "|         4.8|        3.0|         1.4|        0.3|    setosa|[4.80000019073486...|[-1.2599635705956...|  2.0|\n",
      "|         4.8|        3.4|         1.6|        0.2|    setosa|[4.80000019073486...|[-1.2599635705956...|  2.0|\n",
      "|         4.8|        3.4|         1.9|        0.2|    setosa|[4.80000019073486...|[-1.2599635705956...|  2.0|\n",
      "|         4.9|        2.4|         3.3|        1.0|versicolor|[4.90000009536743...|[-1.1392003814247...|  0.0|\n",
      "|         4.9|        2.5|         4.5|        1.7| virginica|[4.90000009536743...|[-1.1392003814247...|  1.0|\n",
      "|         4.9|        3.0|         1.4|        0.2|    setosa|[4.90000009536743...|[-1.1392003814247...|  2.0|\n",
      "|         4.9|        3.1|         1.5|        0.1|    setosa|[4.90000009536743...|[-1.1392003814247...|  2.0|\n",
      "|         4.9|        3.1|         1.5|        0.1|    setosa|[4.90000009536743...|[-1.1392003814247...|  2.0|\n",
      "|         4.9|        3.1|         1.5|        0.1|    setosa|[4.90000009536743...|[-1.1392003814247...|  2.0|\n",
      "|         5.0|        2.0|         3.5|        1.0|versicolor|   [5.0,2.0,3.5,1.0]|[-1.0184371922538...|  0.0|\n",
      "|         5.0|        2.3|         3.3|        1.0|versicolor|[5.0,2.2999999523...|[-1.0184371922538...|  0.0|\n",
      "+------------+-----------+------------+-----------+----------+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train test split\n",
    "train, test = string_species_iris.randomSplit([0.8, 0.2], seed=1234)\n",
    "\n",
    "train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|label|\n",
      "+----------+-----+\n",
      "|       2.0|  2.0|\n",
      "|       2.0|  2.0|\n",
      "|       2.0|  2.0|\n",
      "|       2.0|  2.0|\n",
      "|       2.0|  2.0|\n",
      "|       2.0|  2.0|\n",
      "|       2.0|  2.0|\n",
      "|       2.0|  2.0|\n",
      "|       1.0|  0.0|\n",
      "|       2.0|  2.0|\n",
      "|       2.0|  2.0|\n",
      "|       0.0|  0.0|\n",
      "|       1.0|  1.0|\n",
      "|       0.0|  0.0|\n",
      "|       1.0|  0.0|\n",
      "|       1.0|  0.0|\n",
      "|       1.0|  1.0|\n",
      "|       0.0|  0.0|\n",
      "|       1.0|  1.0|\n",
      "|       1.0|  1.0|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lr.fit(train).transform(test).select([\"prediction\", \"label\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Wrap the Logistic regression model flow above by PipeLine.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|label|\n",
      "+----------+-----+\n",
      "|       2.0|  2.0|\n",
      "|       2.0|  2.0|\n",
      "|       2.0|  2.0|\n",
      "|       2.0|  2.0|\n",
      "|       2.0|  2.0|\n",
      "|       2.0|  2.0|\n",
      "|       2.0|  2.0|\n",
      "|       2.0|  2.0|\n",
      "|       1.0|  0.0|\n",
      "|       2.0|  2.0|\n",
      "|       2.0|  2.0|\n",
      "|       0.0|  0.0|\n",
      "|       1.0|  1.0|\n",
      "|       0.0|  0.0|\n",
      "|       1.0|  0.0|\n",
      "|       1.0|  0.0|\n",
      "|       1.0|  1.0|\n",
      "|       0.0|  0.0|\n",
      "|       1.0|  1.0|\n",
      "|       1.0|  1.0|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "train, test = iris_data.randomSplit([0.8, 0.2], seed=1234)\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler_lr, scaler, string_indexer, lr])\n",
    "\n",
    "model_lr = pipeline.fit(train)\n",
    "model_lr.transform(test).select([\"prediction\", \"label\"]).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
